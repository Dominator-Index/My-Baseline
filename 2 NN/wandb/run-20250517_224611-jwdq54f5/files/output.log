Step 0: Loss = 84.11822509765625
Step 1: Loss = 58.08803939819336
/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1273.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Step 2: Loss = 18.7189998626709
Step 3: Loss = 105.52288055419922
Step 4: Loss = 19.766035079956055
Step 5: Loss = 10.080781936645508
Step 6: Loss = 96.22063446044922
Step 7: Loss = 46.09092330932617
Step 8: Loss = 9.450366020202637
Step 9: Loss = 5.4562907218933105
Step 10: Loss = 2.3740451335906982
Step 11: Loss = 10.687926292419434
Step 12: Loss = 6.510784149169922
Step 13: Loss = 5.400783061981201
Step 14: Loss = 36.5474853515625
Step 15: Loss = 1418.0540771484375
Step 16: Loss = 132703.109375
Step 17: Loss = 419048062976.0
Step 18: Loss = 1.7768652670226564e+31
Step 19: Loss = inf
Step 20: Loss = nan
Step 21: Loss = nan
Step 22: Loss = nan
Step 23: Loss = nan
Step 24: Loss = nan
Step 25: Loss = nan
Step 26: Loss = nan
Step 27: Loss = nan
Step 28: Loss = nan
Step 29: Loss = nan
Step 30: Loss = nan
Step 31: Loss = nan
Step 32: Loss = nan
Step 33: Loss = nan
Step 34: Loss = nan
Step 35: Loss = nan
Step 36: Loss = nan
Step 37: Loss = nan
Step 38: Loss = nan
Step 39: Loss = nan
Step 40: Loss = nan
Step 41: Loss = nan
Step 42: Loss = nan
Step 43: Loss = nan
Step 44: Loss = nan
Step 45: Loss = nan
Step 46: Loss = nan
Step 47: Loss = nan
Step 48: Loss = nan
Step 49: Loss = nan
Step 50: Loss = nan
Step 51: Loss = nan
Step 52: Loss = nan
Step 53: Loss = nan
Step 54: Loss = nan
Step 55: Loss = nan
Step 56: Loss = nan
Step 57: Loss = nan
Step 58: Loss = nan
Step 59: Loss = nan
Step 60: Loss = nan
Step 61: Loss = nan
Step 62: Loss = nan
Step 63: Loss = nan
Step 64: Loss = nan
Step 65: Loss = nan
Step 66: Loss = nan
Step 67: Loss = nan
Step 68: Loss = nan
Step 69: Loss = nan
Step 70: Loss = nan
Traceback (most recent call last):
  File "/home/ouyangzl/BaseLine/2 NN/Top_1_eigenvalue.py", line 151, in <module>
    hessian_eigenvalues = compute_hessian_eigenvalues_pyhessian(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/BaseLine/2 NN/hessian_utils.py", line 94, in compute_hessian_eigenvalues_pyhessian
    hessian_eigen= hessian_computer.eigenvalues(maxIter=1000, tol=1e-8, top_n=top_k, )
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/pyhessian/hessian.py", line 139, in eigenvalues
    tmp_eigenvalue, Hv = self.dataloader_hv_product(v)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/pyhessian/hessian.py", line 108, in dataloader_hv_product
    eigenvalue = group_product(THv, v).cpu().item()
                 ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/pyhessian/utils.py", line 27, in group_product
    def group_product(xs, ys):

KeyboardInterrupt
