Step 0: Loss = 83.96148681640625
Step 1: Loss = 57.92048263549805
/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1273.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Step 2: Loss = 18.66803550720215
Step 3: Loss = 104.9943618774414
Step 4: Loss = 19.442075729370117
Step 5: Loss = 10.50510025024414
Step 6: Loss = 91.6877670288086
Step 7: Loss = 37.71036911010742
Step 8: Loss = 11.81278133392334
Step 9: Loss = 5.000980377197266
Step 10: Loss = 2.3916687965393066
Step 11: Loss = 7.6204705238342285
Step 12: Loss = 3.874925374984741
Step 13: Loss = 27.522754669189453
Step 14: Loss = 271.3056335449219
Step 15: Loss = 577.8869018554688
Step 16: Loss = 103831.9453125
Step 17: Loss = 203129290752.0
Step 18: Loss = 2.0237439376550736e+30
Step 19: Loss = inf
Step 20: Loss = nan
Step 21: Loss = nan
Step 22: Loss = nan
Step 23: Loss = nan
Step 24: Loss = nan
Step 25: Loss = nan
Step 26: Loss = nan
Step 27: Loss = nan
Step 28: Loss = nan
Step 29: Loss = nan
Step 30: Loss = nan
Step 31: Loss = nan
Step 32: Loss = nan
Step 33: Loss = nan
Step 34: Loss = nan
Step 35: Loss = nan
Step 36: Loss = nan
Step 37: Loss = nan
Step 38: Loss = nan
Step 39: Loss = nan
Step 40: Loss = nan
Traceback (most recent call last):
  File "/home/ouyangzl/BaseLine/2 NN/Top_1_eigenvalue.py", line 151, in <module>
    hessian_eigenvalues = compute_hessian_eigenvalues_pyhessian(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/BaseLine/2 NN/hessian_utils.py", line 94, in compute_hessian_eigenvalues_pyhessian
    hessian_eigen= hessian_computer.eigenvalues(maxIter=1000, tol=1e-8, top_n=top_k, )
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/pyhessian/hessian.py", line 139, in eigenvalues
    tmp_eigenvalue, Hv = self.dataloader_hv_product(v)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/pyhessian/hessian.py", line 96, in dataloader_hv_product
    Hv = torch.autograd.grad(gradsH,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/torch/autograd/__init__.py", line 502, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
