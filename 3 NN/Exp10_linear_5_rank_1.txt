ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°: /home/ouyangzl/BaseLine/Exp10/images
Total parameters: 3904
Learning rate is: 1.0
Variance is: 0.01
Computing top 200 Hessian eigenvalues (åŸå§‹å€¼ï¼Œä¸å½’ä¸€åŒ–)
Rank is: 1
ç‰¹å¾å€¼è®¡ç®—é—´éš”: æ¯ 10 æ­¥è®¡ç®—ä¸€æ¬¡
Using SGD optimizer
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.                                                                                                    [1m[34mswanlab[0m[0m: swanlab version 0.6.4 is available!  Upgrade: `pip install -U swanlab`
[1m[34mswanlab[0m[0m: \ Getting project...[1m[34mswanlab[0m[0m: | Getting project...                                                                                                    [1m[34mswanlab[0m[0m: \ Creating experiment...[1m[34mswanlab[0m[0m: | Creating experiment...                                                                                                    [1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.6.3
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/home/ouyangzl/BaseLine/3 NN/swanlog/run-20250620_013749-e31e63f8[0m[0m
[1m[34mswanlab[0m[0m: ğŸ‘‹ Hi [1m[39mcollapsar[0m[0m, welcome to swanlab!
[1m[34mswanlab[0m[0m: Syncing run [33mLinear5+SGD+lr1.0+var0.010000_rank1_top200_raw[0m to the cloud
[1m[34mswanlab[0m[0m: ğŸ  View project at [34m[4mhttps://swanlab.cn/@collapsar/Baseline[0m[0m
[1m[34mswanlab[0m[0m: ğŸš€ View run at [34m[4mhttps://swanlab.cn/@collapsar/Baseline/runs/p36ax6hd8cw07j7wbdyvc[0m[0m
å°†åœ¨ä»¥ä¸‹æ­¥éª¤è®¡ç®—ç‰¹å¾å€¼: [0, 1, 2, 3, 4, 5, 10, 20, 30, 40]...
æ€»å…± 56 ä¸ªè®¡ç®—ç‚¹

============================================================
ğŸš€ å¼€å§‹è®­ç»ƒ - 2025-06-20 01:37:50
æ€»æ­¥æ•°: 501, Seeds: [12138]
ç‰¹å¾å€¼è®¡ç®—æ­¥éª¤: 56 ä¸ª
ç‰¹å¾å€¼ç±»å‹: åŸå§‹å€¼ï¼ˆä¸å½’ä¸€åŒ–ï¼‰
============================================================


ğŸŒ± Seed 12138 (1/1)
ğŸ“Š ç”Ÿæˆä½ç§©çŸ©é˜µ:
   å½¢çŠ¶: torch.Size([10, 16])
   æœŸæœ›ç§©: 1
   å®é™…ç§©: 1
   è®¾å¤‡: cuda
ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰rank=1çš„ä½ç§©çŸ©é˜µä½œä¸ºç›®æ ‡
ğŸ“ æ•°æ®ç»´åº¦: x=torch.Size([16, 16]), y=torch.Size([10, 16])
ğŸ“Š ç”Ÿæˆä½ç§©çŸ©é˜µ:
   å½¢çŠ¶: torch.Size([10, 16])
   æœŸæœ›ç§©: 1
   å®é™…ç§©: 1
   è®¾å¤‡: cuda

ğŸ“Š Low Rank Matrix (rank=1):
tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
       device='cuda:0')
çŸ©é˜µå½¢çŠ¶: torch.Size([10, 16])
å®é™…ç§©: 1
ğŸ“ åŸå§‹æ•°æ®ç»´åº¦: data=torch.Size([16, 16]), label=torch.Size([10, 16])
ğŸ“ è°ƒæ•´åæ•°æ®ç»´åº¦: data=torch.Size([1, 16, 16]), label=torch.Size([1, 10, 16])
Seed 12138:   0%|                                                                               | 0/501 [00:00<?, ?it/s]/home/ouyangzl/.conda/envs/Baseline/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1273.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
