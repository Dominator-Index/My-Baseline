import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import wandb
import time
from datetime import datetime
from tqdm import tqdm  # è¿›åº¦æ¡åº“
from traditional_config import config, device

from model import LinearNetwork
from traditional_data_utils import generate_low_rank_identity, generative_dataset
from sam import SAM
import matplotlib.pyplot as plt  # Add matplotlib for plotting
import seaborn as sns
import sys
import os
import argparse
from hessian_utils import compute_hessian_eigenvalues_pyhessian, compute_hessian_eigen, compute_layer_weight_eigenvalues
from torch.utils.data import TensorDataset, DataLoader
from Top_k_Dominant_Dim_Search import find_dominant_subspace_dimension
# Set arguments
def parse_args():
    parser = argparse.ArgumentParser(description="Train a model with different optimizers")
    
    # Use standard optimizer (SGD, Adam, etc.)
    parser.add_argument('--use_optimizer', action='store_true', default=True, help='ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨')
    parser.add_argument('--use_sam', action='store_true',  help='ä½¿ç”¨SAMä¼˜åŒ–å™¨')
    parser.add_argument('--use_adam', action='store_true', help='ä½¿ç”¨Adamä¼˜åŒ–å™¨')
    parser.add_argument('--threshold', type=float, default=0.98, help='å­ç©ºé—´ç›¸ä¼¼åº¦é˜ˆå€¼')
    parser.add_argument('--rho', type=float, default=0.05, help='SAMçš„rhoå‚æ•°')
    
    # æ–°å¢é—´éš”å‚æ•°
    parser.add_argument('--eigenvalue_interval', type=int, default=10, 
                       help='è®¡ç®—ç‰¹å¾å€¼çš„é—´éš”æ­¥æ•° (é»˜è®¤æ¯10æ­¥è®¡ç®—ä¸€æ¬¡)')
    
    return parser.parse_args()

# Parse arguments
args = parse_args()

# åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
model = LinearNetwork(config["input_dim"], config["hidden_dim"], config["output_dim"], config["variance"], device).to(device)

# Print the number of parameters in the model
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")

# Learning Rate
lr = config["learning_rate"]
print(f"Learning rate is: {lr}")

# Variance 
var = model.var
print(f"Variance is: {var}")

# è·å– top_k å‚æ•°
top_k = config["top_k_pca_number"]
print(f"Computing top {top_k} Hessian eigenvalues")

# è®¡ç®—é—´éš”è®¾ç½®
eigenvalue_interval = args.eigenvalue_interval
print(f"ç‰¹å¾å€¼è®¡ç®—é—´éš”: æ¯ {eigenvalue_interval} æ­¥è®¡ç®—ä¸€æ¬¡")

# Choose optimizer based on arguments
if args.use_sam:
    # Use SAM optimizer
    base_optimizer = torch.optim.SGD # Defined for updating "sharpness-aware" 
    optimizer = SAM(model.parameters(), base_optimizer, lr=config["learning_rate"], momentum=0.9, rho=args.rho, adaptive=False)
    method = "SAM"
    print("Using SAM optimizer")
elif args.use_adam:
    optimizer = torch.optim.Adam(model.parameters(), lr=config["learning_rate"])
    method = "Adam"
    print("Using Adam optimizer")
elif args.use_optimizer:
    optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å­¦ä¹ ç‡
    method = "SGD"
    print("Using SGD optimizer")
else:
    # Default optimizer (you can choose a fallback)
    optimizer = optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)
    method = "GD"
    
wandb.login(key="zrVzavwSxtY7Gs0GWo9xV")
# åˆå§‹åŒ– wandb
wandb.init(project=config["wandb_project_name"], name=f"3NN+GD+{lr}+{var}+{method}_top{top_k}",)

# ä½¿ç”¨ config å­—å…¸æ›´æ–° wandb.config
wandb.config.update(config)
wandb.config.update({"eigenvalue_interval": eigenvalue_interval})  # è®°å½•é—´éš”å‚æ•°

loss_function = nn.MSELoss(reduction='mean')

# è®­ç»ƒè¿‡ç¨‹
steps = config["steps"]

# æ ¹æ®é—´éš”åŠ¨æ€ç”Ÿæˆè®¡ç®—ç‰¹å¾å€¼çš„æ­¥éª¤
important_early_steps = [1, 2, 3, 4, 5]  # å‰å‡ æ­¥å¾ˆé‡è¦
interval_steps = list(range(0, steps + 1, eigenvalue_interval))  # é—´éš”æ­¥éª¤
final_steps = [steps] if steps not in interval_steps else []  # ç¡®ä¿åŒ…å«æœ€åä¸€æ­¥

selected_steps = sorted(set(important_early_steps + interval_steps + final_steps))
print(f"å°†åœ¨ä»¥ä¸‹æ­¥éª¤è®¡ç®—ç‰¹å¾å€¼: {selected_steps[:10]}{'...' if len(selected_steps) > 10 else ''}")
print(f"æ€»å…± {len(selected_steps)} ä¸ªè®¡ç®—ç‚¹")

def set_seed(seed):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.2f}s"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = seconds % 60
        return f"{mins}m {secs:.1f}s"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours}h {mins}m {secs:.1f}s"

def plot_eigenvalue_evolution(eigenvalue_history, step_history, top_k, method, lr, var, seed):
    """ç»˜åˆ¶ç‰¹å¾å€¼æ¼”åŒ–å›¾"""
    if len(step_history) == 0:
        print("âš ï¸  æ²¡æœ‰æ•°æ®ç”¨äºç»˜å›¾")
        return
        
    plt.figure(figsize=(14, 10))
    
    # è®¾ç½®é¢œè‰²æ˜ å°„ - ä½¿ç”¨æ›´å¤šé¢œè‰²é€‰é¡¹
    if top_k <= 10:
        colors = plt.cm.tab10(np.linspace(0, 1, top_k))
    elif top_k <= 20:
        colors = plt.cm.tab20(np.linspace(0, 1, top_k))
    else:
        # å¯¹äºæ›´å¤šç‰¹å¾å€¼ï¼Œä½¿ç”¨æ¸å˜è‰²
        colors = plt.cm.viridis(np.linspace(0, 1, top_k))
    
    # ç»˜åˆ¶æ¯ä¸ªç‰¹å¾å€¼çš„æ¼”åŒ–
    valid_lines = 0
    for i in range(min(len(eigenvalue_history), top_k)):
        key = f"top_{i+1}"
        if key in eigenvalue_history and len(eigenvalue_history[key]) > 0:
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            data_len = len(eigenvalue_history[key])
            steps_for_this_data = step_history[:data_len]
            
            plt.plot(steps_for_this_data, 
                    eigenvalue_history[key], 
                    color=colors[i], 
                    linewidth=2 if i < 5 else 1.5,  # å‰5ä¸ªç‰¹å¾å€¼ç”¨ç²—çº¿
                    alpha=0.9 if i < 10 else 0.7,   # å‰10ä¸ªç‰¹å¾å€¼æ›´æ˜¾çœ¼
                    label=f'Î»{i+1}',
                    marker='o' if len(steps_for_this_data) < 50 and i < 10 else None,
                    markersize=4 if i < 5 else 3)
            valid_lines += 1
    
    plt.xlabel('Training Step', fontsize=14)
    plt.ylabel('Eigenvalue', fontsize=14)
    plt.title(f'Evolution of Top {top_k} Hessian Eigenvalues\n{method} Optimizer, LR={lr}, Variance={var:.6f}, Seed={seed}', 
              fontsize=16)
    
    # æ”¹è¿›å›¾ä¾‹æ˜¾ç¤º
    if valid_lines <= 20:
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, ncol=1)
    else:
        # å¯¹äºå¤ªå¤šç‰¹å¾å€¼ï¼Œåªæ˜¾ç¤ºå‰20ä¸ªçš„å›¾ä¾‹
        handles, labels = plt.gca().get_legend_handles_labels()
        plt.legend(handles[:20], labels[:20], bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9, ncol=2)
    
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦ï¼Œæ›´å®¹æ˜“çœ‹åˆ°åˆ†å‰
    
    # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plot_filename = f"eigenvalue_evolution_{method}_lr{lr}_var{var:.6f}_seed{seed}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç‰¹å¾å€¼æ¼”åŒ–å›¾å·²ä¿å­˜: {plot_filename}")
    
    # ä¹Ÿè®°å½•åˆ° wandb
    wandb.log({"Eigenvalue_Evolution_Plot": wandb.Image(plot_filename)})
    
    # æ˜¾ç¤ºå›¾ç‰‡ï¼ˆå¦‚æœåœ¨ Jupyter ä¸­è¿è¡Œï¼‰
    plt.show()
    plt.close()

def plot_eigenvalue_divergence_analysis(eigenvalue_history, step_history, top_k, method, lr, var, seed):
    """åˆ†æç‰¹å¾å€¼åˆ†å‰ç‚¹"""
    if len(step_history) < 2:
        print("âš ï¸  æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†å‰åˆ†æ")
        return
        
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 16))
    
    # è®¾ç½®é¢œè‰²
    if top_k <= 10:
        colors = plt.cm.tab10(np.linspace(0, 1, top_k))
    elif top_k <= 20:
        colors = plt.cm.tab20(np.linspace(0, 1, top_k))
    else:
        colors = plt.cm.viridis(np.linspace(0, 1, top_k))
    
    # ä¸Šå›¾ï¼šåŸå§‹ç‰¹å¾å€¼æ¼”åŒ– (æ˜¾ç¤ºå‰10ä¸ª)
    display_count = min(10, top_k)
    for i in range(display_count):
        key = f"top_{i+1}"
        if key in eigenvalue_history and len(eigenvalue_history[key]) > 0:
            data_len = len(eigenvalue_history[key])
            steps_for_this_data = step_history[:data_len]
            ax1.plot(steps_for_this_data, 
                    eigenvalue_history[key], 
                    color=colors[i], 
                    linewidth=2, 
                    label=f'Î»{i+1}')
    
    ax1.set_ylabel('Eigenvalue (log scale)')
    ax1.set_title(f'Top {display_count} Hessian Eigenvalue Evolution')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    # ä¸­å›¾ï¼šç›¸é‚»ç‰¹å¾å€¼çš„æ¯”å€¼ï¼ˆç”¨äºæ£€æµ‹åˆ†å‰ï¼‰
    ratio_count = min(5, top_k-1)  # æ˜¾ç¤ºå‰5ä¸ªæ¯”å€¼
    for i in range(ratio_count):
        key1 = f"top_{i+1}"
        key2 = f"top_{i+2}"
        if (key1 in eigenvalue_history and key2 in eigenvalue_history and 
            len(eigenvalue_history[key1]) > 0 and len(eigenvalue_history[key2]) > 0):
            
            # è®¡ç®—æ¯”å€¼
            min_len = min(len(eigenvalue_history[key1]), len(eigenvalue_history[key2]))
            ratios = [eigenvalue_history[key1][j] / max(eigenvalue_history[key2][j], 1e-10) 
                     for j in range(min_len)]
            
            ax2.plot(step_history[:min_len], ratios, 
                    color=colors[i], linewidth=2, 
                    label=f'Î»{i+1}/Î»{i+2}')
    
    ax2.set_ylabel('Eigenvalue Ratio')
    ax2.set_title('Eigenvalue Ratios (Divergence Detection)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ä¸‹å›¾ï¼šç‰¹å¾å€¼é—´éš™åˆ†æ
    gap_count = min(5, top_k-1)
    for i in range(gap_count):
        key1 = f"top_{i+1}"
        key2 = f"top_{i+2}"
        if (key1 in eigenvalue_history and key2 in eigenvalue_history and 
            len(eigenvalue_history[key1]) > 0 and len(eigenvalue_history[key2]) > 0):
            
            # è®¡ç®—é—´éš™
            min_len = min(len(eigenvalue_history[key1]), len(eigenvalue_history[key2]))
            gaps = [eigenvalue_history[key1][j] - eigenvalue_history[key2][j] 
                   for j in range(min_len)]
            
            ax3.plot(step_history[:min_len], gaps, 
                    color=colors[i], linewidth=2, 
                    label=f'Î»{i+1} - Î»{i+2}')
    
    ax3.set_xlabel('Training Step')
    ax3.set_ylabel('Eigenvalue Gap')
    ax3.set_title('Eigenvalue Gaps (Absolute Differences)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')
    
    plt.tight_layout()
    
    # ä¿å­˜å’Œè®°å½•
    plot_filename = f"eigenvalue_divergence_analysis_{method}_lr{lr}_var{var:.6f}_seed{seed}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    wandb.log({"Eigenvalue_Divergence_Analysis": wandb.Image(plot_filename)})
    
    print(f"ğŸ“ˆ ç‰¹å¾å€¼åˆ†å‰åˆ†æå›¾å·²ä¿å­˜: {plot_filename}")
    plt.show()
    plt.close()

def plot_eigenvalue_spectrum_evolution(eigenvalue_history, step_history, top_k, method, lr, var, seed):
    """ç»˜åˆ¶ç‰¹å¾å€¼è°±çš„æ¼”åŒ–çƒ­å›¾"""
    if len(step_history) < 2:
        return
    
    # å‡†å¤‡æ•°æ®çŸ©é˜µ
    data_matrix = []
    for step_idx, step in enumerate(step_history):
        eigenvals_at_step = []
        for i in range(top_k):
            key = f"top_{i+1}"
            if key in eigenvalue_history and step_idx < len(eigenvalue_history[key]):
                eigenvals_at_step.append(eigenvalue_history[key][step_idx])
            else:
                eigenvals_at_step.append(np.nan)
        data_matrix.append(eigenvals_at_step)
    
    data_matrix = np.array(data_matrix).T  # è½¬ç½®ï¼Œè¡Œä¸ºç‰¹å¾å€¼ï¼Œåˆ—ä¸ºæ—¶é—´æ­¥
    
    # åˆ›å»ºçƒ­å›¾
    plt.figure(figsize=(16, 8))
    
    # ä½¿ç”¨å¯¹æ•°å˜æ¢
    log_data = np.log10(np.maximum(data_matrix, 1e-10))
    
    # åˆ›å»ºçƒ­å›¾
    im = plt.imshow(log_data, cmap='viridis', aspect='auto', 
                   extent=[step_history[0], step_history[-1], top_k, 1])
    
    plt.colorbar(im, label='logâ‚â‚€(Eigenvalue)')
    plt.xlabel('Training Step')
    plt.ylabel('Eigenvalue Rank')
    plt.title(f'Hessian Eigenvalue Spectrum Evolution\n{method} Optimizer, LR={lr}, Variance={var:.6f}')
    
    # è®¾ç½®yè½´åˆ»åº¦
    plt.yticks(np.arange(1, min(top_k+1, 21), 2))
    
    plt.tight_layout()
    
    # ä¿å­˜
    plot_filename = f"eigenvalue_spectrum_{method}_lr{lr}_var{var:.6f}_seed{seed}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    wandb.log({"Eigenvalue_Spectrum_Evolution": wandb.Image(plot_filename)})
    
    print(f"ğŸŒˆ ç‰¹å¾å€¼è°±æ¼”åŒ–å›¾å·²ä¿å­˜: {plot_filename}")
    plt.show()
    plt.close()

# seeds = [42, 100, 500, 2000, 12138]
seeds = [12138]  # For reproducibility, you can use a single seed or multiple seeds

# è®°å½•æ€»è®­ç»ƒå¼€å§‹æ—¶é—´
total_start_time = time.time()
print(f"\n{'='*60}")
print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ€»æ­¥æ•°: {steps+1}, Seeds: {seeds}")
print(f"ç‰¹å¾å€¼è®¡ç®—æ­¥éª¤: {len(selected_steps)} ä¸ª")
print(f"{'='*60}\n")

for seed_idx, seed in enumerate(seeds):
    # è®¾ç½®éšæœºç§å­
    set_seed(seed)
    
    print(f"\nğŸŒ± Seed {seed} ({seed_idx+1}/{len(seeds)})")
    
    # åˆå§‹åŒ–æ•°æ®æ”¶é›†å˜é‡
    eigenvalue_history = {f"top_{i+1}": [] for i in range(top_k)}
    step_history = []
    
    # Generate fake data
    data, label = generative_dataset(config["input_dim"], config["output_dim"])
    # data, label çš„åŸå§‹å½¢çŠ¶å°±æ˜¯ (input_dim, input_dim) å’Œ (output_dim, input_dim)
    # åªåœ¨æœ€å¤–å±‚åŠ ä¸€ä¸ª batch ç»´åº¦
    data = data.unsqueeze(0).to(device)   # å˜ä¸º (1, input_dim, input_dim)
    label = label.unsqueeze(0).to(device) # å˜ä¸º (1, output_dim, input_dim)

    single_loader = DataLoader(
            TensorDataset(data, label),
            batch_size=1,
            shuffle=False
        )

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(range(steps + 1), 
                       desc=f"Seed {seed}", 
                       ncols=120,
                       bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
    
    # æ—¶é—´ç»Ÿè®¡å˜é‡
    step_times = []
    forward_times = []
    backward_times = []
    hessian_times = []
    logging_times = []

    for step in progress_bar:
        step_start_time = time.time()
        
        # Forward pass
        forward_start = time.time()
        output = model.forward(data)
        loss = loss_function(output, label)
        forward_time = time.time() - forward_start
        forward_times.append(forward_time)
        
        # Backward pass and optimization
        backward_start = time.time()
        if args.use_sam:
            # First forward-backward pass
            loss.backward()
            optimizer.first_step(zero_grad=True)

            # Second forward-backward pass
            output = model(data)
            loss = loss_function(output, label)
            loss.backward()

            optimizer.second_step(zero_grad=True)

        elif args.use_adam:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        elif args.use_optimizer:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        else:
            grads = torch.autograd.grad(loss, model.parameters(), create_graph=True, retain_graph=True)
            with torch.no_grad():
                for param, grad in zip(model.parameters(), grads):
                    param.data.copy_(param - 0.01 * grad)  # é¿å… in-place ä¿®æ”¹
        backward_time = time.time() - backward_start
        backward_times.append(backward_time)
        
        # å‡†å¤‡è®°å½•å­—å…¸ï¼Œå§‹ç»ˆåŒ…å«æŸå¤±
        log_dict = {"Training Loss": loss.item()}
        
        # ç‰¹å¾å€¼è®¡ç®—
        hessian_time = 0
        if step in selected_steps:
            hessian_start = time.time()
            
            # è®¡ç®— Hessian çš„å‰ top_k ä¸ªç‰¹å¾å€¼
            try:
                eigenvalues = compute_hessian_eigenvalues_pyhessian(
                    model=model,
                    criterion=loss_function,
                    data_loader=single_loader,
                    top_k=top_k,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ top_k_pca_number
                    device=device
                )
                
                # æ”¶é›†æ•°æ®ç”¨äºæœ¬åœ°ç»˜å›¾
                step_history.append(step)
                for i, eigenval in enumerate(eigenvalues):
                    eigenvalue_history[f"top_{i+1}"].append(eigenval.item())
                
                # ä½¿ç”¨åˆ†ç»„è®°å½•ç‰¹å¾å€¼ - è¿™æ ·ä¼šåœ¨åŒä¸€å¼ å›¾ä¸­æ˜¾ç¤º
                hessian_eigenvals = {}
                for i, eigenval in enumerate(eigenvalues):
                    # ä½¿ç”¨ "/" åˆ›å»ºåˆ†ç»„ï¼Œè¿™æ ·ä¼šåœ¨åŒä¸€å¼ å›¾ä¸­æ˜¾ç¤º
                    hessian_eigenvals[f"Hessian_Eigenvalues/Top_{i+1}"] = eigenval.item()
                
                # è®°å½•åˆ° log_dict
                log_dict.update(hessian_eigenvals)
                
                # ä¹Ÿè®°å½•æœ€å¤§ç‰¹å¾å€¼ï¼ˆä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼‰
                log_dict[f"max_hessian_auto_{method}_{lr}_{var}_{seed}"] = eigenvalues[0].item()
                
            except Exception as e:
                tqdm.write(f"âš ï¸  Step {step}: è®¡ç®— Hessian ç‰¹å¾å€¼å¤±è´¥: {e}")
            
            # è®¡ç®—æ¯ä¸€å±‚æƒé‡çŸ©é˜µçš„ç‰¹å¾å€¼ - ä¹Ÿä½¿ç”¨åˆ†ç»„
            try:
                layer_eigenvalues, layer_eigenvectors = compute_layer_weight_eigenvalues(model, top_k=1)
                
                # ä½¿ç”¨åˆ†ç»„è®°å½•å±‚ç‰¹å¾å€¼
                layer_eigenvals = {}
                for layer_name, eigenvals in layer_eigenvalues.items():
                    max_eigenval = eigenvals[0]  # æœ€å¤§ç‰¹å¾å€¼
                    layer_eigenvals[f"Layer_Eigenvalues/{layer_name}"] = max_eigenval
                
                # è®°å½•åˆ° log_dict
                log_dict.update(layer_eigenvals)
                    
            except Exception as e:
                tqdm.write(f"âš ï¸  Step {step}: è®¡ç®—å±‚ç‰¹å¾å€¼å¤±è´¥: {e}")
            
            hessian_time = time.time() - hessian_start
            hessian_times.append(hessian_time)
        
        # Logging
        logging_start = time.time()
        wandb.log(log_dict, step=step)
        logging_time = time.time() - logging_start
        logging_times.append(logging_time)
        
        # è®¡ç®—å•æ­¥æ€»æ—¶é—´
        step_time = time.time() - step_start_time
        step_times.append(step_time)
        
        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
        avg_step_time = np.mean(step_times[-10:])  # æœ€è¿‘10æ­¥çš„å¹³å‡æ—¶é—´
        remaining_steps = steps - step
        eta = avg_step_time * remaining_steps
        
        # æ˜¾ç¤ºä¿¡æ¯
        postfix_dict = {
            'Loss': f'{loss.item():.4f}',
            'Time': f'{step_time:.2f}s'
        }
        
        if step in selected_steps:
            postfix_dict['EigenTime'] = f'{hessian_time:.1f}s'
        
        postfix_dict['ETA'] = format_time(eta)
        
        progress_bar.set_postfix(postfix_dict)
        
        # æ¯éš”ä¸€å®šæ­¥æ•°æ‰“å°è¯¦ç»†ä¿¡æ¯
        if step % 50 == 0 or step == steps or step in [1, 5, 10]:
            current_time = datetime.now().strftime('%H:%M:%S')
            eigenvalue_info = f" | EigenTime: {hessian_time:.2f}s" if step in selected_steps else ""
            tqdm.write(f"ğŸ“Š Step {step:3d} | {current_time} | Loss: {loss.item():.6f} | Time: {step_time:.2f}s{eigenvalue_info}")
            
            if step > 0 and step % 50 == 0:
                total_elapsed = time.time() - total_start_time
                avg_hessian_time = np.mean(hessian_times) if hessian_times else 0
                tqdm.write(f"   ğŸ“ˆ Avg Step: {np.mean(step_times):.2f}s | Avg Hessian: {avg_hessian_time:.2f}s | Total: {format_time(total_elapsed)}")

    # è®­ç»ƒå®Œæˆåçš„ç»Ÿè®¡ä¿¡æ¯
    progress_bar.close()
    
    print(f"\nâœ… Seed {seed} å®Œæˆ!")
    print(f"ğŸ“Š æ—¶é—´ç»Ÿè®¡:")
    print(f"   å¹³å‡æ¯æ­¥æ—¶é—´: {np.mean(step_times):.3f}s")
    print(f"   Forwardå¹³å‡: {np.mean(forward_times):.3f}s")
    print(f"   Backwardå¹³å‡: {np.mean(backward_times):.3f}s") 
    if hessian_times:
        print(f"   Hessianå¹³å‡: {np.mean(hessian_times):.3f}s")
        print(f"   ç‰¹å¾å€¼è®¡ç®—æ¬¡æ•°: {len(hessian_times)}")
    print(f"   Loggingå¹³å‡: {np.mean(logging_times):.3f}s")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: {format_time(sum(step_times))}")
    
    # ç”Ÿæˆæœ¬åœ°å›¾è¡¨
    print(f"\nğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    if len(step_history) > 0:
        try:
            # ç”ŸæˆåŸºæœ¬æ¼”åŒ–å›¾
            plot_eigenvalue_evolution(eigenvalue_history, step_history, top_k, method, lr, var, seed)
            
            # ç”Ÿæˆåˆ†å‰åˆ†æå›¾
            plot_eigenvalue_divergence_analysis(eigenvalue_history, step_history, top_k, method, lr, var, seed)
            
            # ç”Ÿæˆè°±æ¼”åŒ–çƒ­å›¾
            plot_eigenvalue_spectrum_evolution(eigenvalue_history, step_history, top_k, method, lr, var, seed)
            
            print(f"âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå®Œæˆ!")
            
        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
    else:
        print(f"âš ï¸  æ²¡æœ‰æ”¶é›†åˆ°ç‰¹å¾å€¼æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")

# å®Œæˆè®­ç»ƒ
total_time = time.time() - total_start_time
print(f"\nğŸ‰ æ‰€æœ‰è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {format_time(total_time)}")
print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° wandb: {wandb.run.url}")

# å®Œæˆ wandb è¿è¡Œ
wandb.finish()