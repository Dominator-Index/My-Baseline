import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import swanlab as wandb
import time
from datetime import datetime
from tqdm import tqdm
from config_linear import training_config as config
from config_linear import device

from model_linear import LinearNetwork
from generate_low_rank import generate_low_rank_matrix, generative_dataset
from sam import SAM
import matplotlib.pyplot as plt
import os
import argparse
from hessian_utils import compute_hessian_eigenvalues_pyhessian
from torch.utils.data import TensorDataset, DataLoader

# åˆ›å»ºå›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹
IMAGE_SAVE_DIR = "/home/ouyangzl/BaseLine/3 NN/images"
os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)
print(f"ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°: {IMAGE_SAVE_DIR}")

# Set arguments
def parse_args():
    parser = argparse.ArgumentParser(description="Train a model with different optimizers")
    
    parser.add_argument('--use_optimizer', action='store_true', default=True, help='ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨')
    parser.add_argument('--use_sam', action='store_true', help='ä½¿ç”¨SAMä¼˜åŒ–å™¨')
    parser.add_argument('--use_adam', action='store_true', help='ä½¿ç”¨Adamä¼˜åŒ–å™¨')
    parser.add_argument('--rho', type=float, default=0.05, help='SAMçš„rhoå‚æ•°')
    parser.add_argument('--eigenvalue_interval', type=int, default=10, 
                       help='è®¡ç®—ç‰¹å¾å€¼çš„é—´éš”æ­¥æ•° (é»˜è®¤æ¯10æ­¥è®¡ç®—ä¸€æ¬¡)')
    
    return parser.parse_args()

# Parse arguments
args = parse_args()

# åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
model = LinearNetwork(config["input_dim"], config["hidden_dim"], config["output_dim"], 3, config["variance"], device).to(device)

# Print the number of parameters in the model
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")

# Learning Rate
lr = config["learning_rate"]
print(f"Learning rate is: {lr}")

# Variance 
var = model.var
print(f"Variance is: {var}")

# è·å– top_k å‚æ•°
top_k = config["top_k_pca_number"]
print(f"Computing top {top_k} Hessian eigenvalues (åŸå§‹å€¼ï¼Œä¸å½’ä¸€åŒ–)")

# Get the rank from config
rank = config.get("rank", 5)  # é»˜è®¤å€¼ä¸º5
print(f"Rank is: {rank}")

# è®¡ç®—é—´éš”è®¾ç½®
eigenvalue_interval = args.eigenvalue_interval
print(f"ç‰¹å¾å€¼è®¡ç®—é—´éš”: æ¯ {eigenvalue_interval} æ­¥è®¡ç®—ä¸€æ¬¡")

# Choose optimizer based on arguments
if args.use_sam:
    base_optimizer = torch.optim.SGD
    optimizer = SAM(model.parameters(), base_optimizer, lr=config["learning_rate"], momentum=0.9, rho=args.rho, adaptive=False)
    method = "SAM"
    print("Using SAM optimizer")
elif args.use_adam:
    optimizer = torch.optim.Adam(model.parameters(), lr=config["learning_rate"])
    method = "Adam"
    print("Using Adam optimizer")
elif args.use_optimizer:
    optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)
    method = "SGD"
    print("Using SGD optimizer")
else:
    optimizer = optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)
    method = "GD"

# wandb.login(key="19f26ee33b3dd19e282387aa75e310e4b07df17a")
# åˆå§‹åŒ– wandb
wandb.init(project=config["swanlab_project_name"], name=f"3NN+{method}+lr{lr}+var{var:.6f}_rank{rank}_top{top_k}_raw", api_key="zrVzavwSxtY7Gs0GWo9xV")

# ä½¿ç”¨ config å­—å…¸æ›´æ–° wandb.config
wandb.config.update(config)
wandb.config.update({
    "eigenvalue_interval": eigenvalue_interval, 
    "eigenvalue_type": "raw_unnormalized",
    "target_rank": rank,
    "matrix_type": "low_rank_identity"
})

loss_function = nn.MSELoss(reduction='mean')

# è®­ç»ƒè¿‡ç¨‹
steps = config["steps"]

# æ ¹æ®é—´éš”åŠ¨æ€ç”Ÿæˆè®¡ç®—ç‰¹å¾å€¼çš„æ­¥éª¤
important_early_steps = [1, 2, 3, 4, 5]
interval_steps = list(range(0, steps + 1, eigenvalue_interval))
final_steps = [steps] if steps not in interval_steps else []

selected_steps = sorted(set(important_early_steps + interval_steps + final_steps))
print(f"å°†åœ¨ä»¥ä¸‹æ­¥éª¤è®¡ç®—ç‰¹å¾å€¼: {selected_steps[:10]}{'...' if len(selected_steps) > 10 else ''}")
print(f"æ€»å…± {len(selected_steps)} ä¸ªè®¡ç®—ç‚¹")

def set_seed(seed):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.2f}s"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = seconds % 60
        return f"{mins}m {secs:.1f}s"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours}h {mins}m {secs:.1f}s"

def plot_training_loss(loss_history, step_history, method, lr, var, seed, rank):
    """ç»˜åˆ¶è®­ç»ƒæŸå¤±å›¾"""
    if len(loss_history) == 0:
        print("âš ï¸  æ²¡æœ‰æŸå¤±æ•°æ®ç”¨äºç»˜å›¾")
        return
    
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.plot(step_history, loss_history, 'b-', linewidth=2, label='Training Loss')
    
    plt.xlabel('Training Step', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.title(f'Training Loss Evolution\n{method} Optimizer, LR={lr}, Variance={var:.6f}, Rank={rank}, Seed={seed}', 
          fontsize=16)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    final_loss = loss_history[-1]
    min_loss = min(loss_history)
    initial_loss = loss_history[0]
    
    plt.text(0.02, 0.98, 
             f'Initial Loss: {initial_loss:.6f}\nFinal Loss: {final_loss:.6f}\nMin Loss: {min_loss:.6f}\nReduction: {(initial_loss-final_loss)/initial_loss*100:.2f}%', 
             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plot_filename = os.path.join(IMAGE_SAVE_DIR, f"linear_training_loss_{method}_lr{lr}_var{var:.6f}_rank{rank}_seed{seed}_raw.png")
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“‰ è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜: {plot_filename}")
    
    # è®°å½•åˆ° wandb
    wandb.log({"Training_Loss_Plot": wandb.Image(plot_filename)})
    
    # å¦‚æœåœ¨æ— GUIç¯å¢ƒï¼Œæ³¨é‡Šæ‰plt.show()
    # plt.show()
    plt.close()

def plot_top_k_eigenvalues(eigenvalue_history, step_history, top_k, method, lr, var, seed, rank):
    """ç»˜åˆ¶å‰top kä¸ªåŸå§‹ç‰¹å¾å€¼æ¼”åŒ–å›¾ï¼ˆä¸å½’ä¸€åŒ–ï¼‰"""
    if len(step_history) == 0:
        print("âš ï¸  æ²¡æœ‰ç‰¹å¾å€¼æ•°æ®ç”¨äºç»˜å›¾")
        return
        
    plt.figure(figsize=(16, 12))  # å¢å¤§å›¾ç‰‡å°ºå¯¸ä»¥å®¹çº³æ›´å¤šç‰¹å¾å€¼
    
    # è®¾ç½®é¢œè‰²æ˜ å°„
    if top_k <= 10:
        colors = plt.cm.tab10(np.linspace(0, 1, top_k))
    elif top_k <= 20:
        colors = plt.cm.tab20(np.linspace(0, 1, top_k))
    else:
        colors = plt.cm.viridis(np.linspace(0, 1, top_k))
    
    # ç»˜åˆ¶æ¯ä¸ªç‰¹å¾å€¼çš„æ¼”åŒ–
    valid_lines = 0
    eigenvalue_stats = {}
    
    # è®¡ç®—ç‰¹å¾å€¼èŒƒå›´ä»¥ä¾¿æ›´å¥½çš„å¯è§†åŒ–
    all_eigenvals = []
    for i in range(top_k):
        key = f"top_{i+1}"
        if key in eigenvalue_history and len(eigenvalue_history[key]) > 0:
            all_eigenvals.extend(eigenvalue_history[key])
    
    if all_eigenvals:
        max_eigenval = max(all_eigenvals)
        min_eigenval = min(all_eigenvals)
        eigenval_range = max_eigenval - min_eigenval
        print(f"ğŸ“Š ç‰¹å¾å€¼èŒƒå›´: [{min_eigenval:.6f}, {max_eigenval:.6f}], è·¨åº¦: {eigenval_range:.6f}")
    
    for i in range(top_k):
        key = f"top_{i+1}"
        if key in eigenvalue_history and len(eigenvalue_history[key]) > 0:
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            data_len = len(eigenvalue_history[key])
            steps_for_this_data = step_history[:data_len]
            eigenvals = eigenvalue_history[key]
            
            # ç»Ÿè®¡ä¿¡æ¯
            eigenvalue_stats[f'Î»{i+1}'] = {
                'initial': eigenvals[0],
                'final': eigenvals[-1],
                'max': max(eigenvals),
                'min': min(eigenvals),
                'range': max(eigenvals) - min(eigenvals)
            }
            
            # ç»˜åˆ¶ç‰¹å¾å€¼ï¼Œå‰å‡ ä¸ªç”¨æ›´æ˜¾çœ¼çš„æ ·å¼
            plt.plot(steps_for_this_data, 
                    eigenvals, 
                    color=colors[i], 
                    linewidth=3 if i < 3 else (2.5 if i < 10 else 2),  # å‰3ä¸ªæœ€ç²—ï¼Œå‰10ä¸ªè¾ƒç²—
                    alpha=0.9 if i < 10 else 0.7,   # å‰10ä¸ªç‰¹å¾å€¼æ›´æ˜¾çœ¼
                    label=f'Î»{i+1} (åŸå§‹)',
                    marker='o' if len(steps_for_this_data) < 30 and i < 5 else None,
                    markersize=6 if i < 3 else (5 if i < 10 else 4))
            valid_lines += 1
    
    plt.xlabel('Training Step', fontsize=14)
    plt.ylabel('Raw Hessian Eigenvalue (æœªå½’ä¸€åŒ–)', fontsize=14)
    plt.title(f'Evolution of Top {top_k} Raw Hessian Eigenvalues\n{method} Optimizer, LR={lr}, Variance={var:.6f}, Rank={rank}, Seed={seed}', 
          fontsize=16)
    
    # æ”¹è¿›å›¾ä¾‹æ˜¾ç¤º
    if valid_lines <= 15:
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    else:
        # å¯¹äºå¤ªå¤šç‰¹å¾å€¼ï¼Œåªæ˜¾ç¤ºå‰15ä¸ªçš„å›¾ä¾‹
        handles, labels = plt.gca().get_legend_handles_labels()
        plt.legend(handles[:15], labels[:15], bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    
    plt.grid(True, alpha=0.3)
    
    # å¯¹äºåŸå§‹ç‰¹å¾å€¼ï¼Œå…ˆå°è¯•çº¿æ€§å°ºåº¦ï¼Œå¦‚æœè·¨åº¦å¤ªå¤§å†ç”¨å¯¹æ•°å°ºåº¦
    if all_eigenvals and max(all_eigenvals) > 0:
        max_val = max(all_eigenvals)
        min_val = min([v for v in all_eigenvals if v > 0])  # æ’é™¤éæ­£å€¼
        if max_val / min_val > 1000:  # å¦‚æœè·¨åº¦è¶…è¿‡3ä¸ªæ•°é‡çº§ï¼Œä½¿ç”¨å¯¹æ•°å°ºåº¦
            plt.yscale('log')
            print("ğŸ“Š ä½¿ç”¨å¯¹æ•°å°ºåº¦æ˜¾ç¤ºç‰¹å¾å€¼ï¼ˆè·¨åº¦è¾ƒå¤§ï¼‰")
        else:
            print("ğŸ“Š ä½¿ç”¨çº¿æ€§å°ºåº¦æ˜¾ç¤ºç‰¹å¾å€¼")
    
    # æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    if valid_lines > 0:
        # å°†ç¬¬221è¡Œæ”¹ä¸ºï¼š
        stats_text = f'Rank: {rank}\nEigenvalue Count: {valid_lines}\nComputations: {len(step_history)}'
        if valid_lines >= 1:
            first_ev = eigenvalue_stats.get('Î»1', {})
            if first_ev:
                stats_text += f'\nÎ»1: {first_ev["initial"]:.6f} â†’ {first_ev["final"]:.6f}'
                stats_text += f'\nÎ»1 Range: {first_ev["range"]:.6f}'
        
        if valid_lines >= 2:
            second_ev = eigenvalue_stats.get('Î»2', {})
            if second_ev:
                stats_text += f'\nÎ»2: {second_ev["initial"]:.6f} â†’ {second_ev["final"]:.6f}'
        
        plt.text(0.02, 0.02, stats_text, 
                transform=plt.gca().transAxes, fontsize=11, 
                verticalalignment='bottom',
                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plot_filename = os.path.join(IMAGE_SAVE_DIR, f"linear_top{top_k}_eigenvalues_raw_{method}_lr{lr}_var{var:.6f}_rank{rank}_seed{seed}.png")
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Top-{top_k}åŸå§‹ç‰¹å¾å€¼æ¼”åŒ–å›¾å·²ä¿å­˜: {plot_filename}")
    
    # è®°å½•åˆ° wandb
    wandb.log({"Top_K_Raw_Eigenvalues_Plot": wandb.Image(plot_filename)})
    
    # å¦‚æœåœ¨æ— GUIç¯å¢ƒï¼Œæ³¨é‡Šæ‰plt.show()
    # plt.show()
    plt.close()
    
    # æ‰“å°ç‰¹å¾å€¼ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç‰¹å¾å€¼ç»Ÿè®¡ä¿¡æ¯:")
    for i, (name, stats) in enumerate(eigenvalue_stats.items()):
        if i < 5:  # åªæ‰“å°å‰5ä¸ªç‰¹å¾å€¼çš„è¯¦ç»†ä¿¡æ¯
            print(f"   {name}: åˆå§‹={stats['initial']:.6f}, æœ€ç»ˆ={stats['final']:.6f}, å˜åŒ–={stats['range']:.6f}")

# è®­ç»ƒä¸»å¾ªç¯
seeds = [12138]

# è®°å½•æ€»è®­ç»ƒå¼€å§‹æ—¶é—´
total_start_time = time.time()
print(f"\n{'='*60}")
print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ€»æ­¥æ•°: {steps+1}, Seeds: {seeds}")
print(f"ç‰¹å¾å€¼è®¡ç®—æ­¥éª¤: {len(selected_steps)} ä¸ª")
print(f"ç‰¹å¾å€¼ç±»å‹: åŸå§‹å€¼ï¼ˆä¸å½’ä¸€åŒ–ï¼‰")
print(f"{'='*60}\n")

for seed_idx, seed in enumerate(seeds):
    # è®¾ç½®éšæœºç§å­
    set_seed(seed)
    
    print(f"\nğŸŒ± Seed {seed} ({seed_idx+1}/{len(seeds)})")
    
    # åˆå§‹åŒ–æ•°æ®æ”¶é›†å˜é‡
    eigenvalue_history = {f"top_{i+1}": [] for i in range(top_k)}
    step_history = []  # æ‰€æœ‰æ­¥éª¤ï¼ˆç”¨äºæŸå¤±ï¼‰
    eigenvalue_step_history = []  # è®¡ç®—ç‰¹å¾å€¼çš„æ­¥éª¤
    loss_history = []  # æŸå¤±å†å²
    
    # Generate fake data - ä¿®å¤ç»´åº¦é—®é¢˜
    data, label = generative_dataset(config["input_dim"], config["output_dim"], use_custom_rank=True)
    
    # åœ¨è¿™é‡Œæ·»åŠ æ‰“å°ä½ç§©çŸ©é˜µçš„ä»£ç 
    from generate_low_rank import generate_low_rank_matrix
    projection_matrix = generate_low_rank_matrix(config["input_dim"], config["output_dim"])
    print(f"\nğŸ“Š Low Rank Matrix (rank={config['rank']}):")
    print(projection_matrix)
    print(f"çŸ©é˜µå½¢çŠ¶: {projection_matrix.shape}")
    print(f"å®é™…ç§©: {torch.linalg.matrix_rank(projection_matrix)}")
    
    # ç¡®ä¿ç»´åº¦åŒ¹é… - æ ¹æ®è­¦å‘Šä¿¡æ¯è°ƒæ•´
    print(f"ğŸ“ åŸå§‹æ•°æ®ç»´åº¦: data={data.shape}, label={label.shape}")
    
    # è°ƒæ•´æ•°æ®ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹æœŸæœ›
    if data.dim() == 2:  # å¦‚æœæ•°æ®æ˜¯2ç»´ï¼Œæ·»åŠ batchç»´åº¦
        data = data.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    if label.dim() == 2:  # å¦‚æœæ ‡ç­¾æ˜¯2ç»´ï¼Œæ·»åŠ batchç»´åº¦
        label = label.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    
    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    data = data.to(device)
    label = label.to(device)
    
    print(f"ğŸ“ è°ƒæ•´åæ•°æ®ç»´åº¦: data={data.shape}, label={label.shape}")

    single_loader = DataLoader(
            TensorDataset(data, label),
            batch_size=1,
            shuffle=False
        )

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(range(steps + 1), 
                       desc=f"Seed {seed}", 
                       ncols=120,
                       bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')

    for step in progress_bar:
        step_start_time = time.time()
        
        # Forward pass
        output = model.forward(data)
        
        # ç¡®ä¿è¾“å‡ºå’Œæ ‡ç­¾ç»´åº¦åŒ¹é…
        if output.shape != label.shape:
            print(f"âš ï¸  Step {step}: ç»´åº¦ä¸åŒ¹é… - output: {output.shape}, label: {label.shape}")
            # å°è¯•è°ƒæ•´ç»´åº¦
            if output.dim() == 2 and label.dim() == 3:
                output = output.unsqueeze(0)
            elif output.dim() == 3 and label.dim() == 2:
                label = label.unsqueeze(0)
                
            # å¦‚æœè°ƒæ•´åä»ç„¶ä¸åŒ¹é…ï¼Œå¼ºåˆ¶reshape
            if output.shape != label.shape:
                print(f"ğŸ”§ å¼ºåˆ¶è°ƒæ•´ç»´åº¦: {output.shape} -> {label.shape}")
                output = output.view(label.shape)
            
            # æœ€ç»ˆéªŒè¯
            if output.shape == label.shape:
                print(f"âœ… ç»´åº¦åŒ¹é…æˆåŠŸ: {output.shape}")
            else:
                print(f"âŒ ç»´åº¦åŒ¹é…å¤±è´¥: output={output.shape}, label={label.shape}")
        
        loss = loss_function(output, label)
        # åªåœ¨ç‰¹å®šæ­¥éª¤æ‰“å°æŸå¤±
        if step in [0, 1, 5, 10] or step % 100 == 0:
            print(f"Step {step}: Loss = {loss.item():.6f}")
        
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„æŸå¤±
        step_history.append(step)
        loss_history.append(loss.item())
        
        # Backward pass and optimization
        if args.use_sam:
            loss.backward()
            optimizer.first_step(zero_grad=True)
            output = model(data)
            if output.shape != label.shape:
                if output.dim() == 2 and label.dim() == 3:
                    output = output.unsqueeze(0)
            loss = loss_function(output, label)
            loss.backward()
            optimizer.second_step(zero_grad=True)
        elif args.use_adam:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        elif args.use_optimizer:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        else:
            # æ‰‹åŠ¨æ¢¯åº¦ä¸‹é™ - é¿å…å†…å­˜æ³„æ¼è­¦å‘Š
            optimizer.zero_grad()
            loss.backward()
            with torch.no_grad():
                for param in model.parameters():
                    if param.grad is not None:
                        param.data -= 0.01 * param.grad
        
        # å‡†å¤‡è®°å½•å­—å…¸
        log_dict = {"Training Loss": loss.item()}
        
        # ç‰¹å¾å€¼è®¡ç®—
        hessian_time = 0
        if step in selected_steps:
            hessian_start = time.time()
            
            # è®°å½•ç‰¹å¾å€¼è®¡ç®—çš„æ­¥éª¤
            eigenvalue_step_history.append(step)
            
            # è®¡ç®— Hessian çš„å‰ top_k ä¸ªç‰¹å¾å€¼ï¼ˆåŸå§‹å€¼ï¼Œä¸å½’ä¸€åŒ–ï¼‰
            try:
                eigenvalues = compute_hessian_eigenvalues_pyhessian(
                    model=model,
                    criterion=loss_function,
                    data_loader=single_loader,
                    top_k=top_k,
                    device=device
                )
                
                # ç¡®è®¤è¿™æ˜¯åŸå§‹ç‰¹å¾å€¼ï¼ˆpyhessianåº“é»˜è®¤è¿”å›åŸå§‹å€¼ï¼‰
                print(f"ğŸ“Š Step {step}: å‰5ä¸ªåŸå§‹ç‰¹å¾å€¼ = {eigenvalues[:5]}")
                
                # æ”¶é›†åŸå§‹ç‰¹å¾å€¼ç”¨äºæœ¬åœ°ç»˜å›¾
                for i, eigenval in enumerate(eigenvalues):
                    # ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾å€¼ï¼Œä¸è¿›è¡Œä»»ä½•å½’ä¸€åŒ–
                    raw_eigenval = eigenval.item()  # ä¿æŒåŸå§‹æ•°å€¼
                    eigenvalue_history[f"top_{i+1}"].append(raw_eigenval)
                
                # ä½¿ç”¨åˆ†ç»„è®°å½•åŸå§‹ç‰¹å¾å€¼åˆ° wandb
                hessian_eigenvals = {}
                for i, eigenval in enumerate(eigenvalues):
                    hessian_eigenvals[f"Raw_Hessian_Eigenvalues/Top_{i+1}"] = eigenval.item()
                
                log_dict.update(hessian_eigenvals)
                log_dict[f"max_raw_hessian_{method}_{lr}_{var}_{seed}"] = eigenvalues[0].item()
                
            except Exception as e:
                tqdm.write(f"âš ï¸  Step {step}: è®¡ç®— Hessian ç‰¹å¾å€¼å¤±è´¥: {e}")
                import traceback
                tqdm.write(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            
            hessian_time = time.time() - hessian_start
        
        # è®°å½•åˆ° wandb
        wandb.log(log_dict, step=step)
        
        # è®¡ç®—å•æ­¥æ€»æ—¶é—´
        step_time = time.time() - step_start_time
        
        # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
        postfix_dict = {
            'Loss': f'{loss.item():.4f}',
            'Time': f'{step_time:.2f}s'
        }
        
        if step in selected_steps:
            postfix_dict['EigenTime'] = f'{hessian_time:.1f}s'
        
        progress_bar.set_postfix(postfix_dict)
        
        # å®šæœŸæ‰“å°ä¿¡æ¯
        if step % 50 == 0 or step == steps or step in [1, 5, 10]:
            current_time = datetime.now().strftime('%H:%M:%S')
            eigenvalue_info = f" | EigenTime: {hessian_time:.2f}s" if step in selected_steps else ""
            tqdm.write(f"ğŸ“Š Step {step:3d} | {current_time} | Loss: {loss.item():.6f} | Time: {step_time:.2f}s{eigenvalue_info}")

    progress_bar.close()
    
    print(f"\nâœ… Seed {seed} å®Œæˆ!")
    
    # ç”Ÿæˆå›¾è¡¨
    print(f"\nğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # 1. è®­ç»ƒæŸå¤±å›¾
        if len(loss_history) > 0:
            plot_training_loss(loss_history, step_history, method, lr, var, seed, rank)
        
        # 2. Top-kåŸå§‹ç‰¹å¾å€¼æ¼”åŒ–å›¾
        if len(eigenvalue_step_history) > 0:
            plot_top_k_eigenvalues(eigenvalue_history, eigenvalue_step_history, top_k, method, lr, var, seed, rank)
        
        print(f"âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ å›¾ç‰‡ä¿å­˜ä½ç½®: {IMAGE_SAVE_DIR}")
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

# å®Œæˆè®­ç»ƒ
total_time = time.time() - total_start_time
print(f"\nğŸ‰ æ‰€æœ‰è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {format_time(total_time)}")
print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° wandb: {wandb.run.url}")
print(f"ğŸ“ æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜åˆ°: {IMAGE_SAVE_DIR}")

# å®Œæˆ wandb è¿è¡Œ
wandb.finish()