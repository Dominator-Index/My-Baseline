from Top_k_Dom_search import search_top_k_dominant_space
import torch
import numpy as np
import pyhessian
import swanlab
import time
import logging
import os
from typing import List
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
from config import training_config, device
from model import LinearNetwork
from traditional_data_utils import generate_low_rank_identity, generative_dataset
from hessian_utils import compute_hessian_eigen 
from set_logger import setup_colored_logging
from set_seeds import set_seeds
from train_visualizer import TrainingVisualizer

def train(model, data, label, single_loader, loss_function, optimizer, steps, record_steps, 
          learning_rate, method, device, threshold, seed, visualizer=None):
    """è®­ç»ƒå‡½æ•°ï¼Œå¢åŠ å¯è§†åŒ–æ”¯æŒ"""
    
    logger = logging.getLogger()
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ - ç§å­: {seed}, æ€»æ­¥æ•°: {steps}")

    progress_bar = tqdm(range(steps + 1), 
                       desc=f"è®­ç»ƒè¿›åº¦ (Seed {seed})", 
                       ncols=120)

    for step in progress_bar:
        output = model(data)
        loss = loss_function(output, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if step in record_steps:
            logger.info(f"ğŸ“Š æ­¥éª¤ {step}: å¼€å§‹è®¡ç®— Hessian ç‰¹å¾å€¼å’Œä¸»å¯¼ç©ºé—´")
            
            try:
                # é‡æ–°è®¡ç®—æŸå¤±ï¼ˆä¸ºäº†è®¡ç®— Hessianï¼‰
                model.zero_grad()
                output = model(data)
                loss_for_hessian = loss_function(output, label)
                
                # è®¡ç®— Hessian ç‰¹å¾å€¼
                logger.info(f"ğŸ”„ æ­¥éª¤ {step}: å¼€å§‹è®¡ç®— Hessian ç‰¹å¾å€¼...")
                hessian_start_time = time.time()
                
                eigenvalues, _ = compute_hessian_eigen(
                    loss=loss_for_hessian,
                    params=model.parameters(),
                    top_k=threshold
                )
                
                hessian_time = time.time() - hessian_start_time
                
                # æœç´¢ä¸»å¯¼ç©ºé—´
                top_k_results, gaps = search_top_k_dominant_space(
                    eigenvalues=eigenvalues.tolist(),
                    method=method
                )
                
                sorted_gaps = sorted(gaps, reverse=True)
                
                # æ›´æ–°å¯è§†åŒ–æ•°æ®
                if visualizer is not None:
                    visualizer.update_data(
                        step=step,
                        eigenvalues=eigenvalues.tolist(),
                        gaps=sorted_gaps,
                        loss=loss.item(),
                        top_k=top_k_results
                    )

                # åˆ›å»ºæ—¥å¿—æ•°æ®
                log_data = {
                    "step": step,  # æ˜ç¡®æ·»åŠ  step
                    "Training_Loss": loss.item(),
                    "Top_k_Dominant_Space": top_k_results,
                    "Hessian_Compute_Time": hessian_time,
                }

                # è®°å½•ç‰¹å¾å€¼ - ä½¿ç”¨ç»Ÿä¸€å‰ç¼€ "Eigenvalue/"
                num_eigenvalues_to_plot = min(threshold, len(eigenvalues))  # æœ€å¤š100ä¸ª
                for i in range(num_eigenvalues_to_plot):
                    log_data[f"Eigenvalue/Î»_{i+1:03d}"] = float(eigenvalues[i])

                # è®°å½•é—´éš™ - ä½¿ç”¨ç»Ÿä¸€å‰ç¼€ "Gap/"
                num_gaps_to_plot = min(threshold, len(sorted_gaps))  # æœ€å¤š50ä¸ªé—´éš™
                for i in range(num_gaps_to_plot):
                    log_data[f"Gap/Gap_{i+1:03d}"] = float(sorted_gaps[i])

                # è®°å½•ç»Ÿè®¡æŒ‡æ ‡
                if len(eigenvalues) > 0:
                    log_data.update({
                        "Stats/Max_Eigenvalue": float(eigenvalues[0]),
                        "Stats/Min_Eigenvalue": float(eigenvalues[-1]),
                        "Stats/Eigenvalue_Sum": float(np.sum(eigenvalues[:num_eigenvalues_to_plot])),
                        "Stats/Eigenvalue_Mean": float(np.mean(eigenvalues[:num_eigenvalues_to_plot])),
                        "Stats/Condition_Number": float(eigenvalues[0] / eigenvalues[-1]) if eigenvalues[-1] != 0 else float('inf'),
                    })

                if sorted_gaps:
                    log_data.update({
                        "Stats/Max_Gap": float(sorted_gaps[0]),
                        "Stats/Gap_Mean": float(np.mean(sorted_gaps[:10])),
                    })

                # ä¸€æ¬¡æ€§è®°å½•æ‰€æœ‰æ•°æ®
                swanlab.log(log_data, step=step)

                logger.info(f"ğŸ¯ Top-k ä¸»å¯¼ç©ºé—´: {top_k_results}")
                logger.info(f"ğŸ“ˆ æœ€å¤§é—´éš™: {sorted_gaps[0] if sorted_gaps else 0:.6f}")
                logger.info(f"ğŸ“ˆ å‰5ä¸ªé—´éš™: {sorted_gaps[:5] if len(sorted_gaps) >= 5 else sorted_gaps}")

            except Exception as e:
                logger.error(f"âŒ æ­¥éª¤ {step} è®¡ç®—å¤±è´¥: {str(e)}")
                logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
                # è‡³å°‘è®°å½•æŸå¤±
                swanlab.log({
                    "step": step,
                    "Training Loss": loss.item(),
                    "Error": str(e)
                })
    
    logger.info("âœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    # è®¾ç½® SwanLab API Key
    os.environ["SWANLAB_API_KEY"] = training_config["swanlab_api_key"]
    
    # è®¾ç½®å½©è‰²æ—¥å¿—
    logger = setup_colored_logging()
    
    logger.info("ğŸ¨ åˆå§‹åŒ–å½©è‰²æ—¥å¿—ç³»ç»Ÿ")
    logger.info("ğŸ”‘ è®¾ç½® SwanLab API Key")
    logger.info("ğŸ“ˆ åˆå§‹åŒ– SwanLab")

    try:
        # åˆå§‹åŒ– swanlab
        swanlab.init(
            project=training_config["swanlab_project_name"],
            workspace="collapsar",
            name=training_config["swanlab_run_name"],
            config={
                "learning_rate": training_config["learning_rate"],
                "input_dim": training_config["input_dim"],
                "hidden_dim": training_config["hidden_dim"],
                "output_dim": training_config["output_dim"],
                "variance": training_config["variance"],
                "top_k_pca_number": training_config["top_k_pca_number"],
                "eigenvalue_interval": training_config["eigenvalue_interval"],
                "method": training_config["method"],
                "num_layer": training_config["num_layer"],
                "device": str(device),
                "epochs": training_config["steps"],
                "record_steps_count": len(training_config["record_steps"])
            }
        )
        logger.info("âœ… SwanLab åˆå§‹åŒ–æˆåŠŸ!")
        
    except Exception as e:
        logger.error(f"âŒ SwanLab åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        logger.error("ğŸ”§ è¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®")
        raise

    # è®¾ç½®éšæœºç§å­
    seed = training_config.get("seed", training_config["torch_seed"])
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: numpy={training_config['np_seed']}, torch={training_config['torch_seed']}")

    logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # æ˜¾ç¤ºè®°å½•ç­–ç•¥ä¿¡æ¯
    record_steps = training_config["record_steps"]
    logger.info(f"ğŸ“Š Hessian è®¡ç®—ç­–ç•¥: ä»…åœ¨æŒ‡å®šæ­¥æ•°è®¡ç®— ({len(record_steps)} ä¸ªæ£€æŸ¥ç‚¹)")
    logger.info(f"ğŸ“ è®°å½•æ­¥æ•°èŒƒå›´: {min(record_steps)} -> {max(record_steps)}, é—´éš”: {training_config['eigenvalue_interval']}")

    logger.info("ğŸ”„ ç”Ÿæˆè®­ç»ƒæ•°æ®")
    data, label = generative_dataset(training_config["input_dim"], training_config["output_dim"])
    data = data.unsqueeze(0).to(device)
    label = label.unsqueeze(0).to(device)
    logger.info(f"ğŸ“¦ æ•°æ®å½¢çŠ¶: data={data.shape}, label={label.shape}")

    single_loader = DataLoader(
        TensorDataset(data, label),
        batch_size=1,
        shuffle=False
    )

    seeds = [42, 43, 44, 45, 46]
    num_layers = [2, 3, 4, 5, 6, 7, 8, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
    

    logger.info("ğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹")
    model = LinearNetwork(
        input_dim=training_config["input_dim"],
        hidden_dim=trianing_config["hidden_dim"],
        output_dim=training_config["output_dim"],
        num_layer=training_config["num_layer"],  
        var=training_config["variance"],  
        device=device,  
    ).to(device)

    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"ğŸ”¢ æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_function = torch.nn.MSELoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=training_config["learning_rate"])
    
    logger.info("ğŸ“Š æŸå¤±å‡½æ•°: MSELoss")
    logger.info(f"âš™ï¸  ä¼˜åŒ–å™¨: SGD, å­¦ä¹ ç‡={training_config['learning_rate']}")

    # è·å–é…ç½®å‚æ•°
    top_k = training_config["top_k_pca_number"]
    logger.info(f"ğŸ”¢ Top-k æ•°é‡: {top_k}")

    # å¼€å§‹è®­ç»ƒ
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ!")
    logger.info("=" * 60)
    start_time = time.time()
    
    try:
        train(
            model=model,
            data=data,
            label=label,
            single_loader=single_loader,
            loss_function=loss_function,
            optimizer=optimizer,  
            steps=training_config["steps"],
            record_steps=record_steps,
            learning_rate=training_config["learning_rate"],
            method=training_config["method"],
            device=device,
            threshold=top_k,
            seed=seed,
        )
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise
    
    end_time = time.time()
    training_time = end_time - start_time

    logger.info("=" * 60)
    logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {training_time:.2f} ç§’")
    logger.info(f"âš¡ å¹³å‡æ¯ä¸ª Hessian è®¡ç®—ç‚¹ç”¨æ—¶: {training_time/len(record_steps):.2f} ç§’")
    logger.info(f"ğŸ’¾ æ€»å…±è®°å½•äº† {len(record_steps)} ä¸ªæ£€æŸ¥ç‚¹")

    # ç»“æŸ swanlab è®°å½•
    try:
        swanlab.finish()
        logger.info("ğŸ“Š SwanLab è®°å½•å·²ä¿å­˜å¹¶ä¸Šä¼ ")
    except Exception as e:
        logger.error(f"âŒ SwanLab è®°å½•ä¿å­˜å¤±è´¥: {str(e)}")

    logger.info("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆ!")