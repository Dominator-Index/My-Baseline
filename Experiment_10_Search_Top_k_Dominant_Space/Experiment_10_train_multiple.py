from Top_k_Dom_search import search_top_k_dominant_space
import torch
import numpy as np
import pyhessian
import swanlab
import time
import logging
import os
from typing import List
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
from config import training_config, device
from model import LinearNetwork
from generate_low_rank import generate_low_rank_identity, generate_low_rank_matrix, generative_dataset
from hessian_utils import compute_hessian_eigen, compute_hessian_eigenvalues_pyhessian
from set_logger import setup_colored_logging
from set_seeds import set_seeds
import torch.nn as nn
from train_visualizer import TrainingVisualizer

def train(model, data, label, single_loader, loss_function, optimizer, steps, record_steps, 
          learning_rate, method, device, threshold, seed, visualizer=None):
    """è®­ç»ƒå‡½æ•°ï¼Œå¢åŠ å¯è§†åŒ–æ”¯æŒ"""
    
    logger = logging.getLogger()
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ - ç§å­: {seed}, æ€»æ­¥æ•°: {steps}")

    progress_bar = tqdm(range(steps + 1), 
                       desc=f"è®­ç»ƒè¿›åº¦ (Seed {seed})", 
                       ncols=120)

    for step in progress_bar:
        output = model(data)
        loss = loss_function(output, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if step in record_steps:
            logger.info(f"ğŸ“Š æ­¥éª¤ {step}: å¼€å§‹è®¡ç®— Hessian ç‰¹å¾å€¼å’Œä¸»å¯¼ç©ºé—´")
            
            try:
                # é‡æ–°è®¡ç®—æŸå¤±ï¼ˆä¸ºäº†è®¡ç®— Hessianï¼‰
                model.zero_grad()
                output = model(data)
                loss_for_hessian = loss_function(output, label)
                
                # è®¡ç®— Hessian ç‰¹å¾å€¼
                logger.info(f"ğŸ”„ æ­¥éª¤ {step}: å¼€å§‹è®¡ç®— Hessian ç‰¹å¾å€¼...")
                hessian_start_time = time.time()
                
                eigenvalues, _ = compute_hessian_eigen(
                    loss=loss_for_hessian,
                    params=model.parameters(),
                    top_k=threshold
                )
                """
                eigenvalues = compute_hessian_eigenvalues_pyhessian(
                    model=model,
                    criterion=loss_function,
                    data_loader=single_loader,
                    top_k=threshold,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ top_k_pca_number
                    device=device
                )
                """
                
                hessian_time = time.time() - hessian_start_time
                
                # æœç´¢ä¸»å¯¼ç©ºé—´
                top_k_results, gaps = search_top_k_dominant_space(
                    eigenvalues=eigenvalues.tolist(),
                    method=method
                )
                
                sorted_gaps = sorted(gaps, reverse=True)
                
                # æ›´æ–°å¯è§†åŒ–æ•°æ®
                if visualizer is not None:
                    visualizer.update_data(
                        step=step,
                        eigenvalues=eigenvalues.tolist(),
                        gaps=sorted_gaps,
                        loss=loss.item(),
                        top_k=top_k_results,
                        hessian_time=hessian_time
                    )

                # åˆ›å»ºæ—¥å¿—æ•°æ®
                log_data = {
                    "step": step,  # æ˜ç¡®æ·»åŠ  step
                    "Training_Loss": loss.item(),
                    "Top_k_Dominant_Space": top_k_results,
                    "Hessian_Compute_Time": hessian_time,
                }

                # è®°å½•ç‰¹å¾å€¼ - ä½¿ç”¨ç»Ÿä¸€å‰ç¼€ "Eigenvalue/"
                num_eigenvalues_to_plot = min(threshold, len(eigenvalues))  # æœ€å¤š100ä¸ª
                for i in range(num_eigenvalues_to_plot):
                    log_data[f"Eigenvalue/Î»_{i+1:03d}"] = float(eigenvalues[i])

                # è®°å½•é—´éš™ - ä½¿ç”¨ç»Ÿä¸€å‰ç¼€ "Gap/"
                num_gaps_to_plot = min(threshold, len(sorted_gaps))  # æœ€å¤š50ä¸ªé—´éš™
                for i in range(num_gaps_to_plot):
                    log_data[f"Gap/Gap_{i+1:03d}"] = float(sorted_gaps[i])

                # è®°å½•ç»Ÿè®¡æŒ‡æ ‡
                if len(eigenvalues) > 0:
                    log_data.update({
                        "Stats/Max_Eigenvalue": float(eigenvalues[0]),
                        "Stats/Min_Eigenvalue": float(eigenvalues[-1]),
                        "Stats/Eigenvalue_Sum": float(np.sum(eigenvalues[:num_eigenvalues_to_plot])),
                        "Stats/Eigenvalue_Mean": float(np.mean(eigenvalues[:num_eigenvalues_to_plot])),
                        "Stats/Condition_Number": float(eigenvalues[0] / eigenvalues[-1]) if eigenvalues[-1] != 0 else float('inf'),
                    })

                if sorted_gaps:
                    log_data.update({
                        "Stats/Max_Gap": float(sorted_gaps[0]),
                        "Stats/Gap_Mean": float(np.mean(sorted_gaps[:10])),
                    })

                # ä¸€æ¬¡æ€§è®°å½•æ‰€æœ‰æ•°æ®
                swanlab.log(log_data, step=step)

                logger.info(f"ğŸ¯ Top-k ä¸»å¯¼ç©ºé—´: {top_k_results}")
                logger.info(f"ğŸ“ˆ æœ€å¤§é—´éš™: {sorted_gaps[0] if sorted_gaps else 0:.6f}")
                logger.info(f"ğŸ“ˆ å‰5ä¸ªé—´éš™: {sorted_gaps[:5] if len(sorted_gaps) >= 5 else sorted_gaps}")

            except Exception as e:
                logger.error(f"âŒ æ­¥éª¤ {step} è®¡ç®—å¤±è´¥: {str(e)}")
                logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
                # è‡³å°‘è®°å½•æŸå¤±
                swanlab.log({
                    "step": step,
                    "Training Loss": loss.item(),
                    "Error": str(e)
                })
    
    logger.info("âœ… è®­ç»ƒå®Œæˆ!")



def run_single_experiment(seed, num_layer, base_config, use_custom_rank=True):
    """
    è¿è¡Œå•ä¸ªå®éªŒï¼Œæ”¯æŒè‡ªå®šä¹‰rank
    
    Args:
        seed (int): éšæœºç§å­
        num_layer (int): ç½‘ç»œå±‚æ•°
        base_config (dict): åŸºç¡€é…ç½®
        use_custom_rank (bool): æ˜¯å¦ä½¿ç”¨configä¸­çš„è‡ªå®šä¹‰rank
    """
    logger = logging.getLogger()
    
    # è®¾ç½®éšæœºç§å­
    set_seeds(seed)
    logger.info(f"ğŸ§ª å¼€å§‹å®éªŒ: ç§å­={seed}, å±‚æ•°={num_layer}")
    
    # åˆ›å»ºå®éªŒç‰¹å®šçš„é…ç½®
    experiment_config = base_config.copy()
    experiment_config["num_layer"] = num_layer
    experiment_config["torch_seed"] = seed
    experiment_config["np_seed"] = seed

    # è·å–rankä¿¡æ¯ç”¨äºå‘½å
    target_rank = experiment_config.get("rank", "default")
    
    # åŠ¨æ€æ›´æ–° SwanLab è¿è¡Œåç§°
    if use_custom_rank:
        experiment_config["swanlab_run_name"] = f"Exp-Seed{seed}-{num_layer}Layer-Rank{target_rank}-Pyhessian"
        plot_dir = f"/home/ouyangzl/BaseLine/Experiment_10_Search_Top_k_Dominant_Space/plots/seed{seed}_layer{num_layer}_rank{target_rank}_Pyhessian"
    else:
        experiment_config["swanlab_run_name"] = f"Exp-Seed{seed}-{num_layer}Layer-RankDefault-Pyhessian"
        plot_dir = f"/home/ouyangzl/BaseLine/Experiment_10_Search_Top_k_Dominant_Space/plots/seed{seed}_layer{num_layer}_rankdefault_Pyhessian"
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    os.makedirs(plot_dir, exist_ok=True)
    visualizer = TrainingVisualizer(save_dir=plot_dir)
    
    # åˆå§‹åŒ– SwanLab
    swanlab.init(
        project=experiment_config["swanlab_project_name"],
        experiment_name=experiment_config["swanlab_run_name"],
        config=experiment_config,
        reinit=True  # å…è®¸é‡æ–°åˆå§‹åŒ–
    )
    
    try:
        # ç”Ÿæˆæ•°æ®ï¼ˆå¸¦è¯¦ç»†ä¿¡æ¯è¾“å‡ºï¼‰
        logger.info("ğŸ”„ ç”Ÿæˆè®­ç»ƒæ•°æ®")
        logger.info(f"ğŸ“Š å®éªŒé…ç½®: input_dim={experiment_config['input_dim']}, output_dim={experiment_config['output_dim']}")
        if use_custom_rank:
            logger.info(f"ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰rank: {target_rank}")

        data, label = generative_dataset(
            experiment_config["input_dim"], 
            experiment_config["output_dim"],
            use_custom_rank=use_custom_rank
        )

        data = data.unsqueeze(0).to(device)
        label = label.unsqueeze(0).to(device)

        # éªŒè¯ç”Ÿæˆçš„æ•°æ®
        actual_rank = torch.linalg.matrix_rank(label.squeeze(0)).item()
        logger.info(f"âœ… æœ€ç»ˆæ ‡ç­¾çŸ©é˜µrankéªŒè¯: {actual_rank}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        single_loader = DataLoader(
            TensorDataset(data, label),
            batch_size=1,
            shuffle=False
        )
        
        # åˆ›å»ºæ¨¡å‹
        logger.info("ğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹")
        model = LinearNetwork(
            input_dim=experiment_config["input_dim"],
            hidden_dim=experiment_config["hidden_dim"],
            output_dim=experiment_config["output_dim"],
            num_layer=num_layer,
            var=experiment_config["variance"],
            device=device,
        ).to(device)
        
        # ç»Ÿè®¡å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"ğŸ”¢ æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        loss_function = nn.MSELoss()

        logger.info("Learning Rate: {}".format(experiment_config["learning_rate"]))
        optimizer = torch.optim.SGD(model.parameters(), lr=experiment_config["learning_rate"])
        
        logger.info(f"ğŸ“Š æŸå¤±å‡½æ•°: {loss_function.__class__.__name__}")
        logger.info(f"âš™ï¸  ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}, å­¦ä¹ ç‡={experiment_config['learning_rate']}")
        logger.info(f"ğŸ”¢ Top-k æ•°é‡: {experiment_config['top_k_pca_number']}")
        
        # å¼€å§‹è®­ç»ƒ
        train(
            model=model,
            data=data,
            label=label,
            single_loader=single_loader,
            loss_function=loss_function,
            optimizer=optimizer,
            steps=experiment_config["steps"],
            record_steps=experiment_config["record_steps"],
            learning_rate=experiment_config["learning_rate"],
            method=experiment_config["method"],
            device=device,
            threshold=experiment_config["top_k_pca_number"],
            seed=seed,
            visualizer=visualizer  # æ·»åŠ å¯è§†åŒ–å™¨
        )
        
        # è®­ç»ƒå®Œæˆåç”Ÿæˆå›¾è¡¨
        logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        visualizer.plot_eigenvalues(seed, num_layer)
        visualizer.plot_gaps(seed, num_layer)
        visualizer.plot_summary(seed, num_layer)
        
        # ä¿å­˜æ•°æ®åˆ°CSV
        logger.info("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶...")
        visualizer.save_data_to_csv(seed, num_layer)

        logger.info(f"âœ… å®éªŒå®Œæˆ: ç§å­={seed}, å±‚æ•°={num_layer}")
        
    except Exception as e:
        logger.error(f"âŒ å®éªŒå¤±è´¥: ç§å­={seed}, å±‚æ•°={num_layer}, é”™è¯¯: {str(e)}")
        raise e
    
    finally:
        # ç»“æŸå½“å‰ SwanLab è¿è¡Œ
        swanlab.finish()

def run_multiple_experiments(use_custom_rank=True):
    """
    è¿è¡Œå¤šä¸ªå®éªŒçš„ä¸»å‡½æ•°
    """
    logger = logging.getLogger()
    
    # å®éªŒè®¾ç½®
    seeds = [12138]
    num_layers = [2, 3, 4, 5, 6, 7, 8]
    
    total_experiments = len(seeds) * len(num_layers)
    rank_info = training_config.get("rank", "default")

    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å®éªŒ")
    logger.info(f"ğŸ“Š å®éªŒçŸ©é˜µ: {len(seeds)} ä¸ªç§å­ Ã— {len(num_layers)} ä¸ªå±‚æ•° = {total_experiments} ä¸ªå®éªŒ")
    logger.info(f"ğŸ¯ ä½¿ç”¨ranké…ç½®: {rank_info if use_custom_rank else 'default(å•ä½çŸ©é˜µ)'}")
    logger.info(f"ğŸŒ± ç§å­åˆ—è¡¨: {seeds}")
    logger.info(f"ğŸ—ï¸  å±‚æ•°åˆ—è¡¨: {num_layers}")
    
    experiment_count = 0
    successful_experiments = 0
    failed_experiments = 0
    
    # åŒé‡å¾ªç¯è¿›è¡Œå®éªŒ
    for seed in seeds:
        for num_layer in num_layers:
            experiment_count += 1
            
            logger.info("="*80)
            logger.info(f"ğŸ§ª å®éªŒ {experiment_count}/{total_experiments}: ç§å­={seed}, å±‚æ•°={num_layer}, rank={rank_info}")
            logger.info("="*80)
            
            try:
                run_single_experiment(seed, num_layer, training_config, use_custom_rank)
                successful_experiments += 1
                logger.info(f"âœ… å®éªŒ {experiment_count} æˆåŠŸå®Œæˆ")
                
            except Exception as e:
                failed_experiments += 1
                logger.error(f"âŒ å®éªŒ {experiment_count} å¤±è´¥: {str(e)}")
                logger.error(f"ğŸ” å¤±è´¥çš„å®éªŒ: ç§å­={seed}, å±‚æ•°={num_layer}, rank={rank_info}")
                continue

    # å®éªŒæ€»ç»“
    logger.info("="*80)
    logger.info("ğŸ¯ æ‰¹é‡å®éªŒå®Œæˆ!")
    logger.info(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
    logger.info(f"   æ€»å®éªŒæ•°: {total_experiments}")
    logger.info(f"   æˆåŠŸ: {successful_experiments}")
    logger.info(f"   å¤±è´¥: {failed_experiments}")
    logger.info(f"   æˆåŠŸç‡: {successful_experiments/total_experiments*100:.1f}%")
    logger.info(f"   ä½¿ç”¨çš„rank: {rank_info}")
    logger.info("="*80)

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    setup_colored_logging()
    logger = logging.getLogger()
    
    logger.info("ğŸ¬ å®éªŒå¼€å§‹")
    logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    logger.info(f"ğŸ“‹ åŸºç¡€é…ç½®: {training_config}")
    
    # æ˜¾ç¤ºå®éªŒè®¡åˆ’
    record_steps = training_config["record_steps"]
    logger.info(f"ğŸ“Š Hessian è®¡ç®—ç­–ç•¥: ä»…åœ¨æŒ‡å®šæ­¥æ•°è®¡ç®— ({len(record_steps)} ä¸ªæ£€æŸ¥ç‚¹)")
    logger.info(f"ğŸ“ è®°å½•æ­¥æ•°èŒƒå›´: {min(record_steps)} -> {max(record_steps)}, é—´éš”: {training_config['eigenvalue_interval']}")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    run_multiple_experiments(use_custom_rank=True)  # ä½¿ç”¨è‡ªå®šä¹‰rank
    
    logger.info("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")