import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from model_3nn import LinearNetwork as Model3NN
from model import LinearNetwork 
import matplotlib.pyplot as plt

def test_sgd_training_equivalence():
    """æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹åœ¨SGDè®­ç»ƒä¸‹æ˜¯å¦ä¿æŒç­‰ä»·"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®­ç»ƒå‚æ•°
    learning_rate = 0.1
    num_steps = 100
    
    # ç”Ÿæˆå›ºå®šçš„è®­ç»ƒæ•°æ®ï¼ˆidentityè¾“å…¥ï¼‰
    torch.manual_seed(123)  # å›ºå®šæ•°æ®ç”Ÿæˆç§å­
    identity_input = torch.eye(16).unsqueeze(0).to(device)  # [1, 16, 16]
    
    # ç”Ÿæˆç›®æ ‡è¾“å‡ºï¼ˆä½ç§©çŸ©é˜µï¼‰
    rank = 3
    target_matrix = torch.zeros(10, 16).to(device)
    target_matrix[:rank, :rank] = torch.eye(rank).to(device)
    target_output = target_matrix.unsqueeze(0)  # [1, 10, 16]
    
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®:")
    print(f"   è¾“å…¥å½¢çŠ¶: {identity_input.shape}")
    print(f"   ç›®æ ‡å½¢çŠ¶: {target_output.shape}")
    print(f"   ç›®æ ‡rank: {torch.linalg.matrix_rank(target_output.squeeze(0)).item()}")
    
    # ========== è®­ç»ƒ3NNæ¨¡å‹ ==========
    print("\nğŸ—ï¸ åˆ›å»ºå¹¶è®­ç»ƒ3NNæ¨¡å‹...")
    torch.manual_seed(42)  # å›ºå®šæ¨¡å‹åˆå§‹åŒ–ç§å­
    np.random.seed(42)
    
    model_3nn = Model3NN(16, 32, 10, 0.01, device).to(device)
    optimizer_3nn = optim.SGD(model_3nn.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()
    
    losses_3nn = []
    outputs_3nn_history = []
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ3NNæ¨¡å‹...")
    for step in range(num_steps):
        optimizer_3nn.zero_grad()
        
        # 3NNæ¨¡å‹çš„è¾“å‡ºä¸éœ€è¦è¾“å…¥
        output_3nn = model_3nn(identity_input)  # [10, 16]
        output_3nn = output_3nn.unsqueeze(0)    # [1, 10, 16] åŒ¹é…ç›®æ ‡ç»´åº¦
        
        loss_3nn = criterion(output_3nn, target_output)
        loss_3nn.backward()
        optimizer_3nn.step()
        
        losses_3nn.append(loss_3nn.item())
        if step % 10 == 0:
            outputs_3nn_history.append(output_3nn.detach().clone())
            print(f"   Step {step:3d}: Loss = {loss_3nn.item():.6f}")
    
    final_output_3nn = output_3nn.detach()
    
    # ========== è®­ç»ƒExperiment_10æ¨¡å‹ ==========
    print("\nğŸ—ï¸ åˆ›å»ºå¹¶è®­ç»ƒExperiment_10æ¨¡å‹...")
    torch.manual_seed(42)  # ç›¸åŒçš„æ¨¡å‹åˆå§‹åŒ–ç§å­
    np.random.seed(42)
    
    model_exp10 = LinearNetwork(16, 32, 10, 3, 0.01, device).to(device)
    optimizer_exp10 = optim.SGD(model_exp10.parameters(), lr=learning_rate)
    
    losses_exp10 = []
    outputs_exp10_history = []
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒExperiment_10æ¨¡å‹...")
    for step in range(num_steps):
        optimizer_exp10.zero_grad()
        
        # Experiment_10æ¨¡å‹éœ€è¦è¾“å…¥
        output_exp10 = model_exp10(identity_input)  # [1, 10, 16]
        
        loss_exp10 = criterion(output_exp10, target_output)
        loss_exp10.backward()
        optimizer_exp10.step()
        
        losses_exp10.append(loss_exp10.item())
        if step % 10 == 0:
            outputs_exp10_history.append(output_exp10.detach().clone())
            print(f"   Step {step:3d}: Loss = {loss_exp10.item():.6f}")
    
    final_output_exp10 = output_exp10.detach()
    
    # ========== æ¯”è¾ƒç»“æœ ==========
    print("\nğŸ“Š è®­ç»ƒç»“æœæ¯”è¾ƒ:")
    
    # æ¯”è¾ƒæœ€ç»ˆæŸå¤±
    final_loss_diff = abs(losses_3nn[-1] - losses_exp10[-1])
    print(f"   æœ€ç»ˆæŸå¤±å·®å¼‚: {final_loss_diff:.2e}")
    
    # æ¯”è¾ƒæœ€ç»ˆè¾“å‡º
    output_diff = torch.abs(final_output_3nn - final_output_exp10).max()
    print(f"   æœ€ç»ˆè¾“å‡ºå·®å¼‚: {output_diff.item():.2e}")
    
    # æ¯”è¾ƒæƒé‡ä¹˜ç§¯
    W_product_3nn = model_3nn.W3 @ model_3nn.W2 @ model_3nn.W1
    weights_exp10 = [layer.weight for layer in model_exp10.layers]
    W_product_exp10 = weights_exp10[2] @ weights_exp10[1] @ weights_exp10[0]
    
    weight_diff = torch.abs(W_product_3nn - W_product_exp10).max()
    print(f"   æƒé‡ä¹˜ç§¯å·®å¼‚: {weight_diff.item():.2e}")
    
    # åˆ¤æ–­æ˜¯å¦ç­‰ä»·
    is_equivalent = (final_loss_diff < 1e-6 and 
                    output_diff < 1e-6 and 
                    weight_diff < 1e-6)
    
    print(f"\nâœ… è®­ç»ƒç­‰ä»·æ€§: {is_equivalent}")
    
    if not is_equivalent:
        print(f"ğŸ” è¯¦ç»†åˆ†æ:")
        print(f"   æŸå¤±ç­‰ä»·: {final_loss_diff < 1e-6}")
        print(f"   è¾“å‡ºç­‰ä»·: {output_diff < 1e-6}")
        print(f"   æƒé‡ç­‰ä»·: {weight_diff < 1e-6}")
    
    # ========== å¯è§†åŒ– ==========
    plt.figure(figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 3, 1)
    plt.plot(losses_3nn, 'b-', label='3NN Model', linewidth=2)
    plt.plot(losses_exp10, 'r--', label='Experiment_10 Model', linewidth=2)
    plt.xlabel('Training Steps')
    plt.ylabel('MSE Loss')
    plt.title('Training Loss Comparison')
    plt.legend()
    plt.yscale('log')
    plt.grid(True)
    
    # æŸå¤±å·®å¼‚
    plt.subplot(1, 3, 2)
    loss_differences = [abs(l1 - l2) for l1, l2 in zip(losses_3nn, losses_exp10)]
    plt.plot(loss_differences, 'g-', linewidth=2)
    plt.xlabel('Training Steps')
    plt.ylabel('|Loss_3NN - Loss_Exp10|')
    plt.title('Loss Difference')
    plt.yscale('log')
    plt.grid(True)
    
    # è¾“å‡ºå·®å¼‚éšæ—¶é—´å˜åŒ–
    plt.subplot(1, 3, 3)
    output_diffs = []
    for out_3nn, out_exp10 in zip(outputs_3nn_history, outputs_exp10_history):
        diff = torch.abs(out_3nn - out_exp10).max().item()
        output_diffs.append(diff)
    
    steps_recorded = list(range(0, num_steps, 10))
    plt.plot(steps_recorded, output_diffs, 'purple', marker='o', linewidth=2)
    plt.xlabel('Training Steps')
    plt.ylabel('Max Output Difference')
    plt.title('Output Difference Over Time')
    plt.yscale('log')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ å›¾è¡¨å·²ä¿å­˜ä¸º 'training_comparison.png'")
    
    return is_equivalent, {
        'losses_3nn': losses_3nn,
        'losses_exp10': losses_exp10,
        'final_output_diff': output_diff.item(),
        'final_weight_diff': weight_diff.item(),
        'final_loss_diff': final_loss_diff
    }

def test_different_inputs():
    """æµ‹è¯•ä¸åŒè¾“å…¥ä¸‹çš„è®­ç»ƒç­‰ä»·æ€§"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒè¾“å…¥ç±»å‹:")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„è¾“å…¥
    test_inputs = {
        'identity': torch.eye(16).unsqueeze(0),
        'random': torch.randn(1, 16, 16),
        'ones': torch.ones(1, 16, 16),
        'scaled_identity': 2.0 * torch.eye(16).unsqueeze(0)
    }
    
    for input_name, test_input in test_inputs.items():
        print(f"\nğŸ“‹ æµ‹è¯•è¾“å…¥ç±»å‹: {input_name}")
        test_input = test_input.to(device)
        
        # åˆ›å»ºç›¸åŒåˆå§‹åŒ–çš„æ¨¡å‹
        torch.manual_seed(42)
        np.random.seed(42)
        model_3nn = Model3NN(16, 32, 10, 0.01, device).to(device)
        
        torch.manual_seed(42)
        np.random.seed(42)
        model_exp10 = LinearNetwork(16, 32, 10, 3, 0.01, device).to(device)
        
        # å‰å‘ä¼ æ’­æ¯”è¾ƒ
        output_3nn = model_3nn(test_input)  # [10, 16]
        output_exp10 = model_exp10(test_input)  # [1, 10, 16]
        
        # æ¯”è¾ƒè¾“å‡º
        if input_name == 'identity':
            # identityæƒ…å†µä¸‹åº”è¯¥å®Œå…¨ç›¸ç­‰
            diff = torch.abs(output_3nn - output_exp10.squeeze(0)).max()
            expected_equal = True
        else:
            # éidentityæƒ…å†µä¸‹å¯èƒ½ä¸åŒ
            diff = torch.abs(output_3nn - output_exp10.squeeze(0)).max()
            expected_equal = False
        
        print(f"   è¾“å‡ºå·®å¼‚: {diff.item():.2e}")
        print(f"   é¢„æœŸç›¸ç­‰: {expected_equal}")
        print(f"   å®é™…ç›¸ç­‰: {diff < 1e-6}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹SGDè®­ç»ƒç­‰ä»·æ€§æµ‹è¯•")
    
    # ä¸»è¦è®­ç»ƒæµ‹è¯•
    is_equivalent, results = test_sgd_training_equivalence()
    
    print("\n" + "="*60)
    
    # ä¸åŒè¾“å…¥æµ‹è¯•
    test_different_inputs()
    
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“:")
    print(f"   SGDè®­ç»ƒç­‰ä»·æ€§: {is_equivalent}")
    print(f"   æœ€ç»ˆè¾“å‡ºå·®å¼‚: {results['final_output_diff']:.2e}")
    print(f"   æœ€ç»ˆæƒé‡å·®å¼‚: {results['final_weight_diff']:.2e}")
    print(f"   æœ€ç»ˆæŸå¤±å·®å¼‚: {results['final_loss_diff']:.2e}")
    
    if is_equivalent:
        print("âœ… ä¸¤ä¸ªæ¨¡å‹åœ¨SGDè®­ç»ƒä¸‹å®Œå…¨ç­‰ä»·ï¼")
    else:
        print("âš ï¸ ä¸¤ä¸ªæ¨¡å‹åœ¨SGDè®­ç»ƒä¸‹å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æã€‚")